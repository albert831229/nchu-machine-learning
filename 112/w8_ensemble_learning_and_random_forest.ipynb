{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMTdb98Z889HgotcH3vb04e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.rc('font', size=14)\n","plt.rc('axes', labelsize=14, titlesize=14)\n","plt.rc('legend', fontsize=14)\n","plt.rc('xtick', labelsize=10)\n","plt.rc('ytick', labelsize=10)"],"metadata":{"id":"8gCiqG3d5dvA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Voting Classifiers\n","Let's build a voting classifier:"],"metadata":{"id":"PaA0kUG75X6H"}},{"cell_type":"code","source":["from sklearn.datasets import make_moons\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","\n","X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","\n","voting_clf = VotingClassifier(\n","    estimators=[\n","        ('lr', LogisticRegression(random_state=42)),\n","        ('rf', RandomForestClassifier(random_state=42)),\n","        ('svc', SVC(random_state=42))\n","    ]\n",")\n","voting_clf.fit(X_train, y_train)"],"metadata":{"id":"dxNlaD0d5YW5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for name, clf in voting_clf.named_estimators_.items():\n","    print(name, \"=\", clf.score(X_test, y_test))"],"metadata":{"id":"nR0KLhlm52hx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["voting_clf.predict(X_test[:1]) # predict the first instance"],"metadata":{"id":"BGwfaqS95_ig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[clf.predict(X_test[:1]) for clf in voting_clf.estimators_]"],"metadata":{"id":"w9VVr-Ui6GYq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["voting_clf.score(X_test, y_test)"],"metadata":{"id":"pa6MtmxW6ZOd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's use soft voting:"],"metadata":{"id":"9VzqRX868BF1"}},{"cell_type":"code","source":["voting_clf.voting = \"soft\"\n","voting_clf.named_estimators[\"svc\"].probability = True\n","voting_clf.fit(X_train, y_train)\n","voting_clf.score(X_test, y_test)"],"metadata":{"id":"g3dTtBH46ccb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bagging and Pasting\n","## Bagging and Pasting in Scikit-Learn\n","\n","Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClassifier and BaggingRegression."],"metadata":{"id":"JD5CQIiLVUoO"}},{"cell_type":"code","source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# bagging if bootstrap=True else pasting\n","# the n_jobs parameter tells Scikit-Learn the number of CPU cores to use for training and predictions(-1 tells Scikit-Learn to use all available cores)\n","bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n","                            max_samples=100, n_jobs=-1, random_state=42)\n","bag_clf.fit(X_train, y_train)"],"metadata":{"id":"RIFJxFnB8I1f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def plot_decision_boundary(clf, X, y, alpha=1.0):\n","    axes=[-1.5, 2.4, -1, 1.5]\n","    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n","                         np.linspace(axes[2], axes[3], 100))\n","    X_new = np.c_[x1.ravel(), x2.ravel()]\n","    y_pred = clf.predict(X_new).reshape(x1.shape)\n","\n","    plt.contourf(x1, x2, y_pred, alpha=0.3 * alpha, cmap='Wistia')\n","    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8 * alpha)\n","    colors = [\"#78785c\", \"#c47b27\"]\n","    markers = (\"o\", \"^\")\n","    for idx in (0, 1):\n","        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n","                 color=colors[idx], marker=markers[idx], linestyle=\"none\")\n","    plt.axis(axes)\n","    plt.xlabel(r\"$x_1$\")\n","    plt.ylabel(r\"$x_2$\", rotation=0)\n","\n","tree_clf = DecisionTreeClassifier(random_state=42)\n","tree_clf.fit(X_train, y_train)\n","\n","fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n","plt.sca(axes[0])\n","plot_decision_boundary(tree_clf, X_train, y_train)\n","plt.title(\"Decision Tree\")\n","plt.sca(axes[1])\n","plot_decision_boundary(bag_clf, X_train, y_train)\n","plt.title(\"Decision Trees with Bagging\")\n","plt.ylabel(\"\")\n","plt.show()"],"metadata":{"id":"1_XDVKoRVZp9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["note: the BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities，which is the case with Decision Trees classifiers."],"metadata":{"id":"Kqw4Z8PtWpdq"}},{"cell_type":"markdown","source":["## Out-of-Bag evaluation"],"metadata":{"id":"2uk8E-AqDtbk"}},{"cell_type":"code","source":["bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n","                            oob_score=True, n_jobs=-1, random_state=42)\n","bag_clf.fit(X_train, y_train)\n","bag_clf.oob_score_"],"metadata":{"id":"XyF05V7FWWlE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bag_clf.oob_decision_function_[:3]  # Out-of-bag estimated probability for first 3 instances"],"metadata":{"id":"0X--5Js6DzK1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","y_pred = bag_clf.predict(X_test)\n","accuracy_score(y_test, y_pred)"],"metadata":{"id":"QBOKcIcfD75g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you randomly draw one instance from a dataset of size _m_, each instance in the dataset obviously has probability 1/_m_ of getting picked, and therefore it has a probability 1 – 1/_m_ of _not_ getting picked. If you draw _m_ instances with replacement, all draws are independent and therefore each instance has a probability (1 – 1/_m_)<sup>_m_</sup> of _not_ getting picked. Now let's use the fact that exp(_x_) is equal to the limit of (1 + _x_/_m_)<sup>_m_</sup> as _m_ approaches infinity. So if _m_ is large, the ratio of out-of-bag instances will be about exp(–1) ≈ 0.37. So roughly 63% (1 – 0.37) will be sampled."],"metadata":{"id":"mc41hjBdHRJY"}},{"cell_type":"code","source":["# extra code – shows how to compute the 63% proba\n","print(1 - (1 - 1 / 1000) ** 1000)\n","print(1 - np.exp(-1))"],"metadata":{"id":"cOvRbfY0EHtT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Random Forests\n","Compared to Decision Tree, the Random Forest algorithm introduces extra randomness when growing trees；**instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features.** This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model."],"metadata":{"id":"AVN0yuZMHdzH"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,\n","                                 n_jobs=-1, random_state=42)\n","rnd_clf.fit(X_train, y_train)\n","y_pred_rf = rnd_clf.predict(X_test)"],"metadata":{"id":"kYfnCYzEHU04"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A Random Forest is equivalent to a bag of decision trees:"],"metadata":{"id":"k8PTeaa_HpXg"}},{"cell_type":"code","source":["bag_clf = BaggingClassifier(\n","    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n","    n_estimators=500, n_jobs=-1, random_state=42)"],"metadata":{"id":"gNYt0wvAHnJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extra code – verifies that the predictions are identical\n","bag_clf.fit(X_train, y_train)\n","y_pred_bag = bag_clf.predict(X_test)\n","np.all(y_pred_bag == y_pred_rf)  # same predictions"],"metadata":{"id":"pBXegMcZHxJ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Feature Importance\n","If you look at a single Decision Tree, **important features are likely to appear closer to the root of the tree, while unimportant features will often appear closer to the leaves (or not at all)**. It is therefore possible to get an estimate of a feature’s importance by computing the **average depth** at which it appears across all trees in the forest. Scikit-Learn computes this automatically for every feature after training. You can access the result using the feature_importances_ variable."],"metadata":{"id":"Zvx_shiggDLa"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","\n","iris = load_iris(as_frame=True)\n","rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n","rnd_clf.fit(iris.data, iris.target)\n","for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n","    print(round(score, 2), name)"],"metadata":{"id":"paXiCr8NIYH_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Boosting\n","## AdaBoost"],"metadata":{"id":"zSGT7gva0SSW"}},{"cell_type":"code","source":["m = len(X_train)\n","\n","fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n","for subplot, learning_rate in ((0, 1), (1, 0.5)):\n","    sample_weights = np.ones(m) / m   # keep track of sample weights\n","    plt.sca(axes[subplot])\n","    for i in range(5):\n","        svm_clf = SVC(C=0.2, gamma=0.6, random_state=42) # regularized SVM\n","        svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m) # consider sample weights\n","        y_pred = svm_clf.predict(X_train)\n","\n","        error_weights = sample_weights[y_pred != y_train].sum()\n","        r = error_weights / sample_weights.sum()\n","        alpha = learning_rate * np.log((1 - r) / r)\n","        sample_weights[y_pred != y_train] *= np.exp(alpha)\n","        sample_weights /= sample_weights.sum()  # normalization step\n","\n","        plot_decision_boundary(svm_clf, X_train, y_train, alpha=0.4)\n","        plt.title(f\"learning_rate = {learning_rate}\")\n","    if subplot == 0:\n","        plt.text(-0.75, -0.95, \"1\", fontsize=16)\n","        plt.text(-1.05, -0.95, \"2\", fontsize=16)\n","        plt.text(1.0, -0.95, \"3\", fontsize=16)\n","        plt.text(-1.45, -0.5, \"4\", fontsize=16)\n","        plt.text(1.36,  -0.95, \"5\", fontsize=16)\n","    else:\n","        plt.ylabel(\"\")\n","\n","plt.show()"],"metadata":{"id":"tK3Z9U89iXYo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following code trains an AdaBoost classifier based on 200 **Decision Stumps** using Scikit-Learn’s AdaBoostClassifier class (as you might expect，there is also an AdaBoostRegressor class). **A Decision Stump is a Decision Tree with max_depth=1** — in other words，a tree composed of a single decision node plus two leaf nodes. This is the default base estimator for the AdaBoostClassifier class:"],"metadata":{"id":"STGDZqk07Vif"}},{"cell_type":"code","source":["from sklearn.ensemble import AdaBoostClassifier\n","\n","ada_clf = AdaBoostClassifier(\n","    DecisionTreeClassifier(max_depth=1), n_estimators=30,\n","    learning_rate=0.5, random_state=42)\n","ada_clf.fit(X_train, y_train)"],"metadata":{"id":"4R6SRtOl0saV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_decision_boundary(ada_clf, X_train, y_train)"],"metadata":{"id":"5oFckYKN2Q6c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Gradient Boosting\n","Let's create a simple quadratic dataset and fit a DecisionTreeRegressor to it:\n","\n"],"metadata":{"id":"Pq7hhKNmAKqz"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.tree import DecisionTreeRegressor\n","\n","np.random.seed(42)\n","X = np.random.rand(100, 1) - 0.5\n","y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3x² + Gaussian noise\n","\n","tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n","tree_reg1.fit(X, y)"],"metadata":{"id":"Vb0E5uO42WJj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's train another decision tree regressor on the residual errors made by the previous predictor:"],"metadata":{"id":"wO4lIUnyAX1G"}},{"cell_type":"code","source":["y2 = y - tree_reg1.predict(X)\n","tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\n","tree_reg2.fit(X, y2)"],"metadata":{"id":"QsNLLGlIARE9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y3 = y2 - tree_reg2.predict(X)\n","tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\n","tree_reg3.fit(X, y3)"],"metadata":{"id":"knOvRHSvAdMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_new = np.array([[-0.4], [0.], [0.5]])\n","sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"],"metadata":{"id":"LHR94cAbAql1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_predictions(regressors, X, y, axes, style,\n","                     label=None, data_style=\"b.\", data_label=None):\n","    x1 = np.linspace(axes[0], axes[1], 500)\n","    y_pred = sum(regressor.predict(x1.reshape(-1, 1))\n","                 for regressor in regressors)\n","    plt.plot(X[:, 0], y, data_style, label=data_label)\n","    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n","    if label or data_label:\n","        plt.legend(loc=\"upper center\")\n","    plt.axis(axes)\n","\n","plt.figure(figsize=(11, 11))\n","\n","plt.subplot(3, 2, 1)\n","plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"g-\",\n","                 label=\"$h_1(x_1)$\", data_label=\"Training set\")\n","plt.ylabel(\"$y$  \", rotation=0)\n","plt.title(\"Residuals and tree predictions\")\n","\n","plt.subplot(3, 2, 2)\n","plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n","                 label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n","plt.title(\"Ensemble predictions\")\n","\n","plt.subplot(3, 2, 3)\n","plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n","                 label=\"$h_2(x_1)$\", data_style=\"k+\",\n","                 data_label=\"Residuals: $y - h_1(x_1)$\")\n","plt.ylabel(\"$y$  \", rotation=0)\n","\n","plt.subplot(3, 2, 4)\n","plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.2, 0.8],\n","                  style=\"r-\", label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n","\n","plt.subplot(3, 2, 5)\n","plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n","                 label=\"$h_3(x_1)$\", data_style=\"k+\",\n","                 data_label=\"Residuals: $y - h_1(x_1) - h_2(x_1)$\")\n","plt.xlabel(\"$x_1$\")\n","plt.ylabel(\"$y$  \", rotation=0)\n","\n","plt.subplot(3, 2, 6)\n","plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y,\n","                 axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n","                 label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n","plt.xlabel(\"$x_1$\")\n","\n","plt.show()"],"metadata":{"id":"tY772RS7AyTc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's try a gradient boosting regressor:"],"metadata":{"id":"lQGSwmi8JoIr"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingRegressor\n","\n","gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n","                                 learning_rate=1.0, random_state=42)\n","gbrt.fit(X, y)"],"metadata":{"id":"OA4T0re1B15f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The impact of number of estimators"],"metadata":{"id":"EjxSbI-DGRnD"}},{"cell_type":"code","source":["gbrt_few = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n","                                 learning_rate=0.05, random_state=42)\n","gbrt_few.fit(X, y)"],"metadata":{"id":"5Cvrgj7xODaW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gbrt_many = GradientBoostingRegressor(max_depth=2, n_estimators=200,\n","                                 learning_rate=0.05, random_state=42)\n","gbrt_many.fit(X, y)"],"metadata":{"id":"oXwa7CvrOykr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n","\n","plt.sca(axes[0])\n","plot_predictions([gbrt_few], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\",\n","                 label=\"Ensemble predictions\")\n","plt.title(f\"learning_rate={gbrt_few.learning_rate}, \"\n","          f\"n_estimators={gbrt_few.n_estimators_}\")\n","plt.xlabel(\"$x_1$\")\n","plt.ylabel(\"$y$\", rotation=0)\n","\n","plt.sca(axes[1])\n","plot_predictions([gbrt_many], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\")\n","plt.title(f\"learning_rate={gbrt_many.learning_rate}, \"\n","          f\"n_estimators={gbrt_many.n_estimators_}\")\n","plt.xlabel(\"$x_1$\")\n","\n","plt.show()"],"metadata":{"id":"JxUb2v7YODuS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finde the optimal estimator numbers through early stopping"],"metadata":{"id":"pN5smhtirPJT"}},{"cell_type":"code","source":["gbrt_best = GradientBoostingRegressor(\n","    max_depth=2, learning_rate=0.05, n_estimators=500,\n","    n_iter_no_change=10, random_state=42) # early stopping\n","gbrt_best.fit(X, y)"],"metadata":{"id":"2WpI_YNYKCiI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gbrt_best.n_estimators_"],"metadata":{"id":"tNZUFfauLU4P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\")\n","plt.title(f\"learning_rate={gbrt_best.learning_rate}, \"\n","          f\"n_estimators={gbrt_best.n_estimators_}\")\n","plt.xlabel(\"$x_1$\")\n","\n","plt.show()"],"metadata":{"id":"FfihkHGkLVh7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ky6tXE9WLgD6"},"execution_count":null,"outputs":[]}]}